<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Underground Notes</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Underground Notes</description>
    <generator>Hugo -- 0.133.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Eliminating Seams: Joining Separate Meshes with Blender</title>
      <link>http://localhost:1313/posts/neck-seam/</link>
      <pubDate>Sat, 10 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/neck-seam/</guid>
      <description>I recently encountered a problem working with 3D avatars: having a custom-generated head mesh, I needed to merge it seamlessly with the avatar&amp;rsquo;s body. And since you can&amp;rsquo;t go around asking 3D artists to do their magic all the time, here I suggest a simple solution for this type of problem.
Figure 1: Separate head and body meshes
To tackle this, one can make use of Blender&amp;rsquo;s proportional editing, and as it turns out, it works pretty well.</description>
    </item>
    <item>
      <title>Text-Guided 3D Face Synthesis: Paper Break-Down</title>
      <link>http://localhost:1313/posts/sds-faceg2e/</link>
      <pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sds-faceg2e/</guid>
      <description>In this post, I want to revisit Score Samping Distillation (SDS) for 3D head synthesis. Having recently found this paper, I discovered several ideas on how to properly generate textures with SDS. In my previous post on this topic I was mentioning that it was particularly complicated to generate clean face textures.
Figure 1: Image from the paper: faceg2e pipeline
Geometry phase First thing to notice is that the pipeline is split into two parts: geometry generation and then texture generation.</description>
    </item>
    <item>
      <title>Using Stable Diffusion To Tune 3D Morphable Model</title>
      <link>http://localhost:1313/posts/sds-head-shape/</link>
      <pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sds-head-shape/</guid>
      <description>Morphable models One of the easiest and common approaches to face shape reconstruction are 3D morphable models (3DMM). For example, the FLAME head model is used left and right in research papers. Among recent works making use of 3DMM that I&amp;rsquo;ve seen, HRN demonstrates quite impressive results.
I&amp;rsquo;m not going to discuss in depth 3DMMs, how they work or how they&amp;rsquo;re built. Let&amp;rsquo;s just mention that with this approach, a 3D shape is reconstructed as a linear combination of basis components.</description>
    </item>
    <item>
      <title>How To Visually Evaluate Image Classifier Using StyleGAN2</title>
      <link>http://localhost:1313/posts/hairstyle-classifier/hairstyle-classifier/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/hairstyle-classifier/hairstyle-classifier/</guid>
      <description>In this article, we’ll be evaluating an image classifier by exploring StyleGAN2 latent space. We’ll look at the performance of a hairstyle classifier that we use in Ready Player Me.
StyleGAN2 is a generative model architecture demonstrating state-of-the-art image generation. Code of the model and pre-trained weights can be found, for example, in this repo, alongside generated examples across various domains.
Figure 1: Examples generated with StyleGAN2
First, what are we going to do?</description>
    </item>
    <item>
      <title>AUV-Net: Paper Break-down, Part 1</title>
      <link>http://localhost:1313/posts/auvnet-2d/auvnet-2d/</link>
      <pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/auvnet-2d/auvnet-2d/</guid>
      <description>Aligning mesh textures with different topologies could be a fantastic feature for any 3D project. This would unlock numerous possibilities, such as texture transfer or novel texture generation. That’s precisely what the AUV-Net paper by Nvidia addresses. Let’s review the paper to understand its primary concepts, implement it from scratch, and see if we can achieve results similar to those in the paper.
Figure 1: Texture transfer with AUV-Net, image from the paper</description>
    </item>
  </channel>
</rss>
