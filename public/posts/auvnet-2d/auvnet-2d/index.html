<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>AUV-Net: Paper Break-down, Part 1 | Underground Notes</title>
<meta name="keywords" content="pca, alignment, paper">
<meta name="description" content="Aligning mesh textures with different topologies could be a fantastic feature for any 3D project. This would unlock numerous possibilities, such as texture transfer or novel texture generation. That’s precisely what the AUV-Net paper by Nvidia addresses. Let’s review the paper to understand its primary concepts, implement it from scratch, and see if we can achieve results similar to those in the paper.
Figure 1: Texture transfer with AUV-Net, image from the paper">
<meta name="author" content="">
<link rel="canonical" href="https://einstalek.github.io/posts/auvnet-2d/auvnet-2d/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b61fcfd045e796a8b2d82921640c6460592faab5e5c2c76d0ba1c0018405cbfa.css" integrity="sha256-th/P0EXnlqiy2CkhZAxkYFkvqrXlwsdtC6HAAYQFy/o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://einstalek.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://einstalek.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://einstalek.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://einstalek.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://einstalek.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://einstalek.github.io/posts/auvnet-2d/auvnet-2d/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="AUV-Net: Paper Break-down, Part 1" />
<meta property="og:description" content="Aligning mesh textures with different topologies could be a fantastic feature for any 3D project. This would unlock numerous possibilities, such as texture transfer or novel texture generation. That’s precisely what the AUV-Net paper by Nvidia addresses. Let’s review the paper to understand its primary concepts, implement it from scratch, and see if we can achieve results similar to those in the paper.
Figure 1: Texture transfer with AUV-Net, image from the paper" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://einstalek.github.io/posts/auvnet-2d/auvnet-2d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-05-18T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="AUV-Net: Paper Break-down, Part 1"/>
<meta name="twitter:description" content="Aligning mesh textures with different topologies could be a fantastic feature for any 3D project. This would unlock numerous possibilities, such as texture transfer or novel texture generation. That’s precisely what the AUV-Net paper by Nvidia addresses. Let’s review the paper to understand its primary concepts, implement it from scratch, and see if we can achieve results similar to those in the paper.
Figure 1: Texture transfer with AUV-Net, image from the paper"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://einstalek.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "AUV-Net: Paper Break-down, Part 1",
      "item": "https://einstalek.github.io/posts/auvnet-2d/auvnet-2d/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "AUV-Net: Paper Break-down, Part 1",
  "name": "AUV-Net: Paper Break-down, Part 1",
  "description": "Aligning mesh textures with different topologies could be a fantastic feature for any 3D project. This would unlock numerous possibilities, such as texture transfer or novel texture generation. That’s precisely what the AUV-Net paper by Nvidia addresses. Let’s review the paper to understand its primary concepts, implement it from scratch, and see if we can achieve results similar to those in the paper.\nFigure 1: Texture transfer with AUV-Net, image from the paper",
  "keywords": [
    "pca", "alignment", "paper"
  ],
  "articleBody": "Aligning mesh textures with different topologies could be a fantastic feature for any 3D project. This would unlock numerous possibilities, such as texture transfer or novel texture generation. That’s precisely what the AUV-Net paper by Nvidia addresses. Let’s review the paper to understand its primary concepts, implement it from scratch, and see if we can achieve results similar to those in the paper.\nFigure 1: Texture transfer with AUV-Net, image from the paper\n2D case To illustrate the main concept of their approach, the authors first demonstrate how this method would work in 2D. Instead of dealing with meshes, we consider the task of aligning face images in various poses (face textures, as they call it in the paper).\nHaving any dataset of aligned faces, it’s easy to generate misaligned ones using cv2.perspectiveTransform.\nFigure 2: Examples of generated misaligned faces\nTo link this task with texture mapping, one could consider training images as square shapes in 2D, and [original images]are their aligned texture images\nIt is suggested to map each pixel’s coordinates into the aligned UV space, and then index the texture image by UV coordinates from that space.\nPCA analogy We take inspiration from classic linear subspace learning methods such as eigenfaces, where a basis is computed via PCA for a set of face images, so that each face is decomposed into a weighted sum of the eigenfaces. Note that PCA works best when the images are aligned. Therefore, if a network is designed to decompose the input images into weighted sums of basis images, and is allowed to deform the input images before the decomposition, the network should learn to align the input images into a canonical pose\".\nThere’s the main idea: build the network in such a way, that it first learns to deform the input (with the UV mapper), and then decomposes the result into a weighted sum of basis components.\nNow, our goal is to learn the UV mapping and a basis set of faces. A separate encoder is used to predict weights for the basis. In this specific case, given a set of 2D points, the Basis Generator will produce a set of N grayscale images (color for each pixel’s coordinate).\nFigure 3: 2D AUV-Net approach from the paper\nResults Training this is straightforward, so let’s proceed directly to the results. We can pass a uniform grid of points to the Basis Generator to obtain the aligned face basis. Here are some samples of grayscale images from the basis obtained for one input image:\nFigure 4: Aligned face basis components\nThe results are reminiscent of the PCA decomposition applied to face images.\nFigure 5: From left to right: input distorted image, reconstruction, aligned reconstruction, original image\nNow we can obtain aligned input faces, but the quality is low because we’re using a weighted sum operation to produce them. To address this issue, the authors propose the following solution:\nWe sample points from the input image, feed those points to the UV mapper to obtain UV coordinates, use the UV coordinates and colors of the sampled points to fill a blank image, and finally inpaint the missing regions.\nBy following the proposed solution, these are the aligned textures I got:\nFigure 6: Aligned images, high resolution\nConclusion The results appear not as impressive as those shown in the paper, but with additional training iterations, the quality should improve.\nIt’s important to notice one detail here: we see that face images are getting aligned, but not exactly in the expected manner. Results are aligned with each other, but they are not aligned with the canonical pose (which would mean removing the initial distortion).\nOne could actually achieve such result, by adding an additional supervision based on face key-points. This is actually done in the 3D case, to ensure a specific UV split.\n",
  "wordCount" : "636",
  "inLanguage": "en",
  "datePublished": "2023-05-18T00:00:00Z",
  "dateModified": "2023-05-18T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://einstalek.github.io/posts/auvnet-2d/auvnet-2d/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Underground Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://einstalek.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://einstalek.github.io/" accesskey="h" title="Underground Notes (Alt + H)">Underground Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://einstalek.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://einstalek.github.io/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://einstalek.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      AUV-Net: Paper Break-down, Part 1
    </h1>
    <div class="post-meta"><span title='2023-05-18 00:00:00 +0000 UTC'>May 18, 2023</span>&nbsp;·&nbsp;3 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#2d-case" aria-label="2D case">2D case</a></li>
                <li>
                    <a href="#pca-analogy" aria-label="PCA analogy">PCA analogy</a></li>
                <li>
                    <a href="#results" aria-label="Results">Results</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Aligning mesh textures with different topologies could be a fantastic feature for any 3D project. This would unlock numerous possibilities, such as texture transfer or novel texture generation. That’s precisely what the <a href="https://research.nvidia.com/labs/toronto-ai/AUV-NET/">AUV-Net paper</a> by Nvidia addresses. Let’s review the paper to understand its primary concepts, implement it from scratch, and see if we can achieve results similar to those in the paper.</p>
<figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/auvnet-2d/animals-3d.webp#center"
         alt="Figure 1: Texture transfer with AUV-Net, image from the paper"/> <figcaption>
            <p>Figure 1: Texture transfer with AUV-Net, image from the paper</p>
        </figcaption>
</figure>

<h3 id="2d-case">2D case<a hidden class="anchor" aria-hidden="true" href="#2d-case">#</a></h3>
<p>To illustrate the main concept of their approach, the authors first demonstrate how this method would work in 2D. Instead of dealing with meshes, we consider the task of aligning face images in various poses (face textures, as they call it in the paper).</p>
<p>Having any dataset of aligned faces, it’s easy to generate misaligned ones using <code>cv2.perspectiveTransform</code>.</p>
<figure class="custom-caption">
    <img loading="lazy" src="/posts/auvnet-2d/1_VEUyk2tEokrAJ9NllKM1Ng.webp"
         alt="Figure 2: Examples of generated misaligned faces"/> <figcaption>
            <p>Figure 2: Examples of generated misaligned faces</p>
        </figcaption>
</figure>

<blockquote>
<p>To link this task with texture mapping, one could consider training images as square shapes in 2D, and [original images]are their aligned texture images</p>
</blockquote>
<p>It is suggested to map each pixel’s coordinates into the aligned UV space, and then index the texture image by UV coordinates from that space.</p>
<h3 id="pca-analogy">PCA analogy<a hidden class="anchor" aria-hidden="true" href="#pca-analogy">#</a></h3>
<blockquote>
<p>We take inspiration from classic linear subspace learning methods such as eigenfaces, where a basis is computed via PCA for a set of face images, so that each face is decomposed into a weighted sum of the eigenfaces. Note that PCA works best when the images are aligned. Therefore, if a network is designed to decompose the input images into weighted sums of basis images, and is allowed to deform the input images before the decomposition, the network should learn to align the input images into a canonical pose&quot;.</p>
</blockquote>
<p><strong><code>There’s the main idea</code></strong>: build the network in such a way, that it <!-- raw HTML omitted -->first learns to deform the input (with the UV mapper), and then decomposes the result into a weighted sum of basis components<!-- raw HTML omitted -->.</p>
<p>Now, our goal is to learn the UV mapping and a basis set of faces. A separate encoder is used to predict weights for the basis. In this specific case, given a set of 2D points, the <code>Basis Generator</code> will produce a set of N grayscale images (color for each pixel’s coordinate).</p>
<figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/auvnet-2d/1_eSSK0nHI7AhPBHgHNWf9lQ.webp#center"
         alt="Figure 3: 2D AUV-Net approach from the paper"/> <figcaption>
            <p>Figure 3: 2D AUV-Net approach from the paper</p>
        </figcaption>
</figure>

<h3 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h3>
<p>Training this is straightforward, so let’s proceed directly to the results. We can pass a uniform grid of points to the <code>Basis Generator</code> to obtain the aligned face basis. Here are some samples of grayscale images from the basis obtained for one input image:</p>
<figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/auvnet-2d/1_tIk1Vcmy6XQXFXiaf3oLpA.webp#center"
         alt="Figure 4: Aligned face basis components"/> <figcaption>
            <p>Figure 4: Aligned face basis components</p>
        </figcaption>
</figure>

<p>The results are reminiscent of the PCA decomposition applied to face images.</p>
<figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/auvnet-2d/1_-4YrCs6M3CIhPJCGDVV0xg.webp#center"
         alt="Figure 5: From left to right: input distorted image, reconstruction, aligned reconstruction, original image"/> <figcaption>
            <p>Figure 5: From left to right: input distorted image, reconstruction, aligned reconstruction, original image</p>
        </figcaption>
</figure>

<p>Now we can obtain aligned input faces, but the quality is low because we’re using a weighted sum operation to produce them. To address this issue, the authors propose the following solution:</p>
<blockquote>
<p>We sample points from the input image, feed those points to the UV mapper to obtain UV coordinates, use the UV coordinates and colors of the sampled points to fill a blank image, and finally inpaint the missing regions.</p>
</blockquote>
<p>By following the proposed solution, these are the aligned textures I got:</p>
<p><figure class="align-center ">
    <img loading="lazy" src="/posts/auvnet-2d/1__ahi0mkwp5BHSwaAGg2Abw.webp#center"/> 
</figure>

<figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/auvnet-2d/1_S2acPd_IYDv8JaG2qaEG9A.webp#center"
         alt="Figure 6: Aligned images, high resolution"/> <figcaption>
            <p>Figure 6: Aligned images, high resolution</p>
        </figcaption>
</figure>
</p>
<h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>The results appear not as impressive as those shown in the paper, but with additional training iterations, the quality should improve.</p>
<p>It’s important to notice one detail here: we see that face images are getting aligned, but not exactly in the expected manner. Results are aligned with each other, but they are not aligned with the canonical pose (which would mean removing the initial distortion).</p>
<p>One could actually achieve such result, by adding an additional supervision based on face key-points. This is actually done in the 3D case, to ensure a specific UV split.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://einstalek.github.io/tags/pca/">Pca</a></li>
      <li><a href="https://einstalek.github.io/tags/alignment/">Alignment</a></li>
      <li><a href="https://einstalek.github.io/tags/paper/">Paper</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://einstalek.github.io/">Underground Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
