<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Using Stable Diffusion To Tune 3D Morphable Model | Underground Notes</title>
<meta name="keywords" content="sds, latent, diffusion, controlnet, 3d, geometry, texture">
<meta name="description" content="Morphable models One of the easiest and common approaches to face shape reconstruction are 3D morphable models (3DMM). For example, the FLAME head model is used left and right in research papers. Among recent works making use of 3DMM that I&rsquo;ve seen, HRN demonstrates quite impressive results.
I&rsquo;m not going to discuss in depth 3DMMs, how they work or how they&rsquo;re built. Let&rsquo;s just mention that with this approach, a 3D shape is reconstructed as a linear combination of basis components.">
<meta name="author" content="">
<link rel="canonical" href="https://einstalek.github.io/posts/sds-head-shape/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b61fcfd045e796a8b2d82921640c6460592faab5e5c2c76d0ba1c0018405cbfa.css" integrity="sha256-th/P0EXnlqiy2CkhZAxkYFkvqrXlwsdtC6HAAYQFy/o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://einstalek.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://einstalek.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://einstalek.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://einstalek.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://einstalek.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://einstalek.github.io/posts/sds-head-shape/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="Using Stable Diffusion To Tune 3D Morphable Model" />
<meta property="og:description" content="Morphable models One of the easiest and common approaches to face shape reconstruction are 3D morphable models (3DMM). For example, the FLAME head model is used left and right in research papers. Among recent works making use of 3DMM that I&rsquo;ve seen, HRN demonstrates quite impressive results.
I&rsquo;m not going to discuss in depth 3DMMs, how they work or how they&rsquo;re built. Let&rsquo;s just mention that with this approach, a 3D shape is reconstructed as a linear combination of basis components." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://einstalek.github.io/posts/sds-head-shape/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-04T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-01-04T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Using Stable Diffusion To Tune 3D Morphable Model"/>
<meta name="twitter:description" content="Morphable models One of the easiest and common approaches to face shape reconstruction are 3D morphable models (3DMM). For example, the FLAME head model is used left and right in research papers. Among recent works making use of 3DMM that I&rsquo;ve seen, HRN demonstrates quite impressive results.
I&rsquo;m not going to discuss in depth 3DMMs, how they work or how they&rsquo;re built. Let&rsquo;s just mention that with this approach, a 3D shape is reconstructed as a linear combination of basis components."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://einstalek.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Using Stable Diffusion To Tune 3D Morphable Model",
      "item": "https://einstalek.github.io/posts/sds-head-shape/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Using Stable Diffusion To Tune 3D Morphable Model",
  "name": "Using Stable Diffusion To Tune 3D Morphable Model",
  "description": "Morphable models One of the easiest and common approaches to face shape reconstruction are 3D morphable models (3DMM). For example, the FLAME head model is used left and right in research papers. Among recent works making use of 3DMM that I\u0026rsquo;ve seen, HRN demonstrates quite impressive results.\nI\u0026rsquo;m not going to discuss in depth 3DMMs, how they work or how they\u0026rsquo;re built. Let\u0026rsquo;s just mention that with this approach, a 3D shape is reconstructed as a linear combination of basis components.",
  "keywords": [
    "sds", "latent", "diffusion", "controlnet", "3d", "geometry", "texture"
  ],
  "articleBody": "Morphable models One of the easiest and common approaches to face shape reconstruction are 3D morphable models (3DMM). For example, the FLAME head model is used left and right in research papers. Among recent works making use of 3DMM that I’ve seen, HRN demonstrates quite impressive results.\nI’m not going to discuss in depth 3DMMs, how they work or how they’re built. Let’s just mention that with this approach, a 3D shape is reconstructed as a linear combination of basis components.\nFigure 1: first components of a 3DMM\nScore Distillation Sampling Now what’s more interesting is how to reconstruct a head shape given some condition, like a photo or a text description. At ReadyPlayerMe, we predict morphable model’s weights from a single face image. One of the alternatives to that (less practical, but more fun) would be using pretrained diffusion model’s prior knowledge in image space, like it’s done in DreamFusion. The method introduced in this paper, Score Distillation Sampling (SDS) permits 3D model parameter tuning using a pretrained diffusion model.\nFigure 2: DreamFusion, SDS pipeline for NeRF\nTo be able to tune a 3D model, we must be able to compute gradients of the diffusion loss from latent space down to the parameters of the 3D model. Let’s take a look at the expression for those gradients:\nWhere x = g(θ) is the render image generated with a 3D model with parameters θ. Noise ε is predicted by UNet (with parameters φ) from latents z after adding noise to them. Among the three derivatives to be used, the first one (UNet Jacobian) is heavy to compute and is proposed to be omitted. Considering that the latents derivative (corresponding to the encoder) is a constant, we get the following expression:\nNow this looks way more usable. So we can compute gradients for 3D model parameters by just back-propagating the additional part, which is the difference between the noise added to the image latents and the noise predicted by the UNet. The rest is automatically computed thanks to torch autograd.\n3DMM tuning In the case of a 3D morphable head model, the parameter space is the vector of the model’s basis weights. I’ll take a custom, very simple head 3DMM, with 50 components, and iteratively tune its weights using some text description. One thing to keep in mind is that SDS brings instability, so clipping gradients will help in this case. Also, to prevent weights explosion I’ll add L2 regularization.\nHere are some examples, with head model evolution and the text that’s being used as a condition for Stable Diffusion’s UNet.\nFigure 3: From left to right: Winston Churchill, Scarlett Johansson, Barack Obama\nEven with the small size of the 3DMM I’m using, I’d say the results look pretty recognizable. We could also create textures for those heads, tuning them directly in the UV space. Although this seems to produce noisier results.\nFigure 4: Scarlett Johansson, texture generated with SDS\nImplementation details Here’s a pseudocode demonstrating how to implement SDS.\n# get text embeddings for your prompt text_embeddings = encode_prompt(prompt) for _ in range(iterations): # render 3D scene render = render_scene() # encode using SD VAE latents = encode_image(render) # We're omitting UNet gradients with torch.no_grad(): # add noise to latents noise = torch.randn_like(latents) latents_noised = add_noise(latents, noise) # predict noise with UNet noise_pred = unet(latents_noised, t, text_embeddings) # classifier free guidance from diffusers noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) noise_pred = noise_pred_uncond + \\ guidance_scale * (noise_pred_text - noise_pred_uncond) # compute gradients w = (1 - scheduler.alphas[t]) grad = w * (noise_pred - noise) # back propagate latents.backward(gradient=grad, retain_graph=True) ",
  "wordCount" : "597",
  "inLanguage": "en",
  "datePublished": "2024-01-04T00:00:00Z",
  "dateModified": "2024-01-04T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://einstalek.github.io/posts/sds-head-shape/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Underground Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://einstalek.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://einstalek.github.io/" accesskey="h" title="Underground Notes (Alt + H)">Underground Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://einstalek.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://einstalek.github.io/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://einstalek.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Using Stable Diffusion To Tune 3D Morphable Model
    </h1>
    <div class="post-meta"><span title='2024-01-04 00:00:00 +0000 UTC'>January 4, 2024</span>&nbsp;·&nbsp;3 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#morphable-models" aria-label="Morphable models">Morphable models</a></li>
                <li>
                    <a href="#score-distillation-sampling" aria-label="Score Distillation Sampling">Score Distillation Sampling</a></li>
                <li>
                    <a href="#3dmm-tuning" aria-label="3DMM tuning">3DMM tuning</a></li>
                <li>
                    <a href="#implementation-details" aria-label="Implementation details">Implementation details</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="morphable-models">Morphable models<a hidden class="anchor" aria-hidden="true" href="#morphable-models">#</a></h3>
<p>One of the easiest and common approaches to face shape reconstruction are 3D morphable models (3DMM). For example, the <a href="https://flame.is.tue.mpg.de/">FLAME</a> head model is used left and right in research papers. Among recent works making use of 3DMM that I&rsquo;ve seen, <a href="https://github.com/youngLBW/HRN">HRN</a> demonstrates quite impressive results.</p>
<p>I&rsquo;m not going to discuss in depth 3DMMs, how they work or how they&rsquo;re built. Let&rsquo;s just mention that with this approach, a 3D shape is reconstructed as a linear combination of basis components.</p>
<figure class="align-center ">
    <img loading="lazy" src="/posts/sds-head-shape/WAIECKR.png#center"
         alt="Figure 1: first components of a 3DMM" width="50%"/> <figcaption>
            <p>Figure 1: first components of a 3DMM</p>
        </figcaption>
</figure>

<h3 id="score-distillation-sampling">Score Distillation Sampling<a hidden class="anchor" aria-hidden="true" href="#score-distillation-sampling">#</a></h3>
<p>Now what&rsquo;s more interesting is <em>how to reconstruct a head shape given some condition</em>, like a photo or a text description. At <a href="https://readyplayer.me/">ReadyPlayerMe</a>, we predict morphable model&rsquo;s weights from a single face image. One of the alternatives to that (less practical, but more fun) would be using pretrained diffusion model&rsquo;s prior knowledge in image space, like it&rsquo;s done in <a href="https://arxiv.org/pdf/2209.14988.pdf">DreamFusion</a>. The method introduced in this paper, Score Distillation Sampling (SDS) permits 3D model parameter tuning using a pretrained diffusion model.</p>
<figure class="align-center ">
    <img loading="lazy" src="/posts/sds-head-shape/2gdV93m.png#center"
         alt="Figure 2: DreamFusion, SDS pipeline for NeRF" width="100%"/> <figcaption>
            <p>Figure 2: DreamFusion, SDS pipeline for NeRF</p>
        </figcaption>
</figure>

<p>To be able to tune a 3D model, we must be able to compute gradients of the diffusion loss from latent space down to the parameters of the 3D model. Let&rsquo;s take a look at the expression for those gradients:</p>
<figure class="align-center ">
    <img loading="lazy" src="/posts/sds-head-shape/fZdaYV1.png#center" width="60%"/> 
</figure>

<p>Where <em>x = g(θ)</em> is the render image generated with a 3D model with parameters <em>θ</em>. Noise <em>ε</em> is predicted by UNet (with parameters φ) from latents z after adding noise to them. Among the three derivatives to be used, the first one (UNet Jacobian) is heavy to compute and is proposed to be omitted. Considering that the latents derivative (corresponding to the encoder) is a constant, we get the following expression:</p>
<figure class="align-center ">
    <img loading="lazy" src="/posts/sds-head-shape/eFdehuc.png#center" width="60%"/> 
</figure>

<p>Now this looks way more usable. So we can compute gradients for 3D model parameters by just back-propagating the additional part, which is the difference between the noise added to the image latents and the noise predicted by the UNet. The rest is automatically computed thanks to torch autograd.</p>
<h3 id="3dmm-tuning">3DMM tuning<a hidden class="anchor" aria-hidden="true" href="#3dmm-tuning">#</a></h3>
<p>In the case of a 3D morphable head model, the parameter space is the vector of the model&rsquo;s basis weights. I&rsquo;ll take a custom, very simple head 3DMM, with 50 components, and iteratively tune its weights using some text description. One thing to keep in mind is that SDS brings instability, so clipping gradients will help in this case. Also, to prevent weights explosion I&rsquo;ll add <em>L2</em> regularization.</p>
<p>Here are some examples, with head model evolution and the text that&rsquo;s being used as a condition for Stable Diffusion&rsquo;s UNet.</p>
<figure class="align-center ">
    <img loading="lazy" src="/posts/sds-head-shape/ezgif-6-079c0b5805.gif#center"
         alt="Figure 3: From left to right: Winston Churchill, Scarlett Johansson, Barack Obama" width="100%"/> <figcaption>
            <p>Figure 3: From left to right: Winston Churchill, Scarlett Johansson, Barack Obama</p>
        </figcaption>
</figure>

<p>Even with the small size of the 3DMM I&rsquo;m using, I&rsquo;d say the results look pretty recognizable. We could also create textures for those heads, tuning them directly in the UV space. Although this seems to produce noisier results.</p>
<figure class="align-center ">
    <img loading="lazy" src="/posts/sds-head-shape/auvGHgK.png#center"
         alt="Figure 4: Scarlett Johansson, texture generated with SDS" width="50%"/> <figcaption>
            <p>Figure 4: Scarlett Johansson, texture generated with SDS</p>
        </figcaption>
</figure>

<h3 id="implementation-details">Implementation details<a hidden class="anchor" aria-hidden="true" href="#implementation-details">#</a></h3>
<p>Here&rsquo;s a pseudocode demonstrating how to implement SDS.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># get text embeddings for your prompt</span>
</span></span><span style="display:flex;"><span>text_embeddings <span style="color:#f92672">=</span> encode_prompt(prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(iterations):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># render 3D scene</span>
</span></span><span style="display:flex;"><span>    render <span style="color:#f92672">=</span> render_scene()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># encode using SD VAE</span>
</span></span><span style="display:flex;"><span>    latents <span style="color:#f92672">=</span> encode_image(render)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># We&#39;re omitting UNet gradients</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># add noise to latents</span>
</span></span><span style="display:flex;"><span>        noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(latents)
</span></span><span style="display:flex;"><span>        latents_noised <span style="color:#f92672">=</span> add_noise(latents, noise)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># predict noise with UNet</span>
</span></span><span style="display:flex;"><span>        noise_pred <span style="color:#f92672">=</span> unet(latents_noised, t, text_embeddings)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># classifier free guidance from diffusers</span>
</span></span><span style="display:flex;"><span>    noise_pred_uncond, noise_pred_text <span style="color:#f92672">=</span> noise_pred<span style="color:#f92672">.</span>chunk(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    noise_pred <span style="color:#f92672">=</span> noise_pred_uncond <span style="color:#f92672">+</span> \
</span></span><span style="display:flex;"><span>        guidance_scale <span style="color:#f92672">*</span> (noise_pred_text <span style="color:#f92672">-</span> noise_pred_uncond)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># compute gradients</span>
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> scheduler<span style="color:#f92672">.</span>alphas[t])
</span></span><span style="display:flex;"><span>    grad <span style="color:#f92672">=</span> w <span style="color:#f92672">*</span> (noise_pred <span style="color:#f92672">-</span> noise)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># back propagate</span>
</span></span><span style="display:flex;"><span>    latents<span style="color:#f92672">.</span>backward(gradient<span style="color:#f92672">=</span>grad, retain_graph<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://einstalek.github.io/tags/sds/">Sds</a></li>
      <li><a href="https://einstalek.github.io/tags/latent/">Latent</a></li>
      <li><a href="https://einstalek.github.io/tags/diffusion/">Diffusion</a></li>
      <li><a href="https://einstalek.github.io/tags/controlnet/">Controlnet</a></li>
      <li><a href="https://einstalek.github.io/tags/3d/">3d</a></li>
      <li><a href="https://einstalek.github.io/tags/geometry/">Geometry</a></li>
      <li><a href="https://einstalek.github.io/tags/texture/">Texture</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://einstalek.github.io/">Underground Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
