<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Text-Guided 3D Face Synthesis: Paper Break-Down | Underground Notes</title>
<meta name="keywords" content="paper, sds, latent, diffusion, controlnet, 3d, geometry, texture">
<meta name="description" content="In this post, I want to revisit Score Samping Distillation (SDS) for 3D head synthesis. Having recently found this paper, I discovered several ideas on how to properly generate textures with SDS. In my previous post on this topic I was mentioning that it was particularly complicated to generate clean face textures.
Figure 1: Image from the paper: faceg2e pipeline
Geometry phase First thing to notice is that the pipeline is split into two parts: geometry generation and then texture generation.">
<meta name="author" content="">
<link rel="canonical" href="https://einstalek.github.io/posts/sds-faceg2e/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b61fcfd045e796a8b2d82921640c6460592faab5e5c2c76d0ba1c0018405cbfa.css" integrity="sha256-th/P0EXnlqiy2CkhZAxkYFkvqrXlwsdtC6HAAYQFy/o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://einstalek.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://einstalek.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://einstalek.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://einstalek.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://einstalek.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://einstalek.github.io/posts/sds-faceg2e/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="Text-Guided 3D Face Synthesis: Paper Break-Down" />
<meta property="og:description" content="In this post, I want to revisit Score Samping Distillation (SDS) for 3D head synthesis. Having recently found this paper, I discovered several ideas on how to properly generate textures with SDS. In my previous post on this topic I was mentioning that it was particularly complicated to generate clean face textures.
Figure 1: Image from the paper: faceg2e pipeline
Geometry phase First thing to notice is that the pipeline is split into two parts: geometry generation and then texture generation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://einstalek.github.io/posts/sds-faceg2e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-01-18T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Text-Guided 3D Face Synthesis: Paper Break-Down"/>
<meta name="twitter:description" content="In this post, I want to revisit Score Samping Distillation (SDS) for 3D head synthesis. Having recently found this paper, I discovered several ideas on how to properly generate textures with SDS. In my previous post on this topic I was mentioning that it was particularly complicated to generate clean face textures.
Figure 1: Image from the paper: faceg2e pipeline
Geometry phase First thing to notice is that the pipeline is split into two parts: geometry generation and then texture generation."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://einstalek.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Text-Guided 3D Face Synthesis: Paper Break-Down",
      "item": "https://einstalek.github.io/posts/sds-faceg2e/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Text-Guided 3D Face Synthesis: Paper Break-Down",
  "name": "Text-Guided 3D Face Synthesis: Paper Break-Down",
  "description": "In this post, I want to revisit Score Samping Distillation (SDS) for 3D head synthesis. Having recently found this paper, I discovered several ideas on how to properly generate textures with SDS. In my previous post on this topic I was mentioning that it was particularly complicated to generate clean face textures.\nFigure 1: Image from the paper: faceg2e pipeline\nGeometry phase First thing to notice is that the pipeline is split into two parts: geometry generation and then texture generation.",
  "keywords": [
    "paper", "sds", "latent", "diffusion", "controlnet", "3d", "geometry", "texture"
  ],
  "articleBody": "In this post, I want to revisit Score Samping Distillation (SDS) for 3D head synthesis. Having recently found this paper, I discovered several ideas on how to properly generate textures with SDS. In my previous post on this topic I was mentioning that it was particularly complicated to generate clean face textures.\nFigure 1: Image from the paper: faceg2e pipeline\nGeometry phase First thing to notice is that the pipeline is split into two parts: geometry generation and then texture generation. Geometry is decoupled from texture in order not to produce textures with geometry details baked in them.\nThe first part is pretty much the same as I described in my previous post. 3DMM parameters are iteratively updated with a grey-scale render (all vertices assigned the same color) being fed into denoising unet. One thing different is scheduling timestamp to linearly decrease during the training.\nWithout proposed view-dependent prompts, I get something like this during the geometry phase:\nFigure 2: From left to right: Scarlett Johansson, Barack Obama, Elon Musk, Leelee Sobieski\nTexture phase During the texture phase, we fix the geometry and tune the texture in the latent space of VAE decoder. In order not to produce cleaner textures, two types of supervision are proposed.\nDepth supervision: use depth-controlnet, by passing to it depth-render of the mesh, to keep awareness of the geometry itself, separated from the texture. This is intended to get rid of geometric details baked into texture, and to “uphold geometry-texture alignment”.\nTexture-prior: train a custom SD model on textures. This custom SD would help to get rid of light baked into textures, making use of learnt texture distribution.\nFirst, I’m just gonna add the first type of supervision, by using a depth-controlnet. Instead of randomly initializing texture latents, I’ll use latents of some fixed texture for each training. Let’s see the results I get:\nFigure 3: From left to right: Scarlett Johansson, Barack Obama, Elon Musk, Leelee Sobieski\nNow, I’m gonna add something similar to the proposed texture-prior, but without having to train my own SD model. Instead, I will use a controlnet model trained on a pair of textures and canny edges in them. Passing a fixed control input to this controlnet, alongside with a prompt like “face texture,” I’ll pass its output to the denoising UNet and get a second SDS gradient tensor (additional to the one obtained with depth-controlnet).\nFigure 4: From left to right: Scarlett Johansson, Barack Obama, Elon Musk, Leelee Sobieski\nNotice how with the second type of supervision added, resulting textures have more even lighting.\n",
  "wordCount" : "426",
  "inLanguage": "en",
  "datePublished": "2024-01-18T00:00:00Z",
  "dateModified": "2024-01-18T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://einstalek.github.io/posts/sds-faceg2e/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Underground Notes",
    "logo": {
      "@type": "ImageObject",
      "url": "https://einstalek.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://einstalek.github.io/" accesskey="h" title="Underground Notes (Alt + H)">Underground Notes</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://einstalek.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://einstalek.github.io/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://einstalek.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Text-Guided 3D Face Synthesis: Paper Break-Down
    </h1>
    <div class="post-meta"><span title='2024-01-18 00:00:00 +0000 UTC'>January 18, 2024</span>&nbsp;·&nbsp;2 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#geometry-phase" aria-label="Geometry phase">Geometry phase</a></li>
                <li>
                    <a href="#texture-phase" aria-label="Texture phase">Texture phase</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In this post, I want to revisit Score Samping Distillation (SDS) for 3D head synthesis. Having recently found <a href="https://faceg2e.github.io/">this paper</a>, I discovered several ideas on how to properly generate textures with SDS. In my previous post on this topic I was mentioning that it was particularly complicated to generate clean face textures.</p>
<figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/sds-faceg2e/OHSYAoA.png#center"
         alt="Figure 1: Image from the paper: faceg2e pipeline"/> <figcaption>
            <p>Figure 1: Image from the paper: faceg2e pipeline</p>
        </figcaption>
</figure>

<h3 id="geometry-phase">Geometry phase<a hidden class="anchor" aria-hidden="true" href="#geometry-phase">#</a></h3>
<p>First thing to notice is that the pipeline is split into two parts: geometry generation and then texture generation. Geometry is decoupled from texture in order not to produce textures with geometry details baked in them.</p>
<p>The first part is pretty much the same as I described in my <a href="sds-head-shape">previous post</a>. 3DMM parameters are iteratively updated with a grey-scale render (all vertices assigned the same color) being fed into denoising unet. One thing different is scheduling timestamp to linearly decrease during the training.</p>
<p>Without proposed view-dependent prompts, I get something like this during the geometry phase:</p>
<figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/sds-faceg2e/ezgif-1-03ff04e486.gif#center"
         alt="Figure 2: From left to right: Scarlett Johansson, Barack Obama, Elon Musk, Leelee Sobieski" width="1024"/> <figcaption>
            <p>Figure 2: From left to right: Scarlett Johansson, Barack Obama, Elon Musk, Leelee Sobieski</p>
        </figcaption>
</figure>

<h3 id="texture-phase">Texture phase<a hidden class="anchor" aria-hidden="true" href="#texture-phase">#</a></h3>
<p>During the texture phase, we fix the geometry and tune the texture in the latent space of VAE decoder. In order not to produce cleaner textures, two types of supervision are proposed.</p>
<ol>
<li>
<p>Depth supervision: use depth-controlnet, by passing to it depth-render of the mesh, to keep awareness of the geometry itself, separated from the texture. This is intended to get rid of geometric details baked into texture, and to &ldquo;uphold geometry-texture alignment&rdquo;.</p>
</li>
<li>
<p>Texture-prior: train a custom SD model on textures. This custom SD would help to get rid of light baked into textures, making use of learnt texture distribution.</p>
</li>
</ol>
<p>First, I&rsquo;m just gonna add the first type of supervision, by using a depth-controlnet. Instead of randomly initializing texture latents, I&rsquo;ll use latents of some fixed texture for each training. Let&rsquo;s see the results I get:</p>
<p><figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/sds-faceg2e/A29xHod.png#center" width="1024"/> 
</figure>

<figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/sds-faceg2e/ezgif-1-970d274084.gif#center"
         alt="Figure 3: From left to right: Scarlett Johansson, Barack Obama, Elon Musk, Leelee Sobieski" width="1024"/> <figcaption>
            <p>Figure 3: From left to right: Scarlett Johansson, Barack Obama, Elon Musk, Leelee Sobieski</p>
        </figcaption>
</figure>
</p>
<p>Now, I&rsquo;m gonna add something similar to the proposed texture-prior, but without having to train my own SD model. Instead, I will use a controlnet model trained on a pair of textures and canny edges in them. Passing a fixed control input to this controlnet, alongside with a prompt like &ldquo;face texture,&rdquo; I&rsquo;ll pass its output to the denoising UNet and get a second SDS gradient tensor (additional to the one obtained with depth-controlnet).</p>
<p><figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/sds-faceg2e/qhgLfpn.png#center" width="1024"/> 
</figure>

<figure class="align-center custom-caption">
    <img loading="lazy" src="/posts/sds-faceg2e/ezgif-1-1809d561e9.gif#center"
         alt="Figure 4: From left to right: Scarlett Johansson, Barack Obama, Elon Musk, Leelee Sobieski" width="1024"/> <figcaption>
            <p>Figure 4: From left to right: Scarlett Johansson, Barack Obama, Elon Musk, Leelee Sobieski</p>
        </figcaption>
</figure>
</p>
<p>Notice how with the second type of supervision added, resulting textures have more even lighting.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://einstalek.github.io/tags/paper/">Paper</a></li>
      <li><a href="https://einstalek.github.io/tags/sds/">Sds</a></li>
      <li><a href="https://einstalek.github.io/tags/latent/">Latent</a></li>
      <li><a href="https://einstalek.github.io/tags/diffusion/">Diffusion</a></li>
      <li><a href="https://einstalek.github.io/tags/controlnet/">Controlnet</a></li>
      <li><a href="https://einstalek.github.io/tags/3d/">3d</a></li>
      <li><a href="https://einstalek.github.io/tags/geometry/">Geometry</a></li>
      <li><a href="https://einstalek.github.io/tags/texture/">Texture</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://einstalek.github.io/">Underground Notes</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
